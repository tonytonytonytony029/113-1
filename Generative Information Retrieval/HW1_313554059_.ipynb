{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query 1: 966 849 730\n",
      "Test Query 2: 145 328 663\n",
      "Test Query 3: 551 932 982\n",
      "Test Query 4: 359 874 111\n",
      "Test Query 5: 973 204 357\n",
      "Test Query 6: 975 688 348\n",
      "Test Query 7: 807 847 794\n",
      "Test Query 8: 299 441 756\n",
      "Test Query 9: 546 885 394\n",
      "Test Query 10: 780 269 691\n",
      "Test Query 11: 443 263 409\n",
      "Test Query 12: 683 543 809\n",
      "Test Query 13: 79 905 20\n",
      "Test Query 14: 881 511 803\n",
      "Test Query 15: 596 684 923\n",
      "Test Query 16: 411 563 726\n",
      "Test Query 17: 360 181 957\n",
      "Test Query 18: 389 340 221\n",
      "Test Query 19: 618 867 734\n",
      "Test Query 20: 394 82 649\n",
      "Test Query 21: 420 835 473\n",
      "Test Query 22: 620 530 300\n",
      "Test Query 23: 735 286 575\n",
      "Test Query 24: 285 804 200\n",
      "Test Query 25: 827 965 894\n",
      "Test Query 26: 244 539 574\n",
      "Test Query 27: 369 631 316\n",
      "Test Query 28: 954 788 969\n",
      "Test Query 29: 353 350 871\n",
      "Test Query 30: 282 570 994\n",
      "Test Query 31: 811 736 885\n",
      "Test Query 32: 208 771 470\n",
      "Test Query 33: 927 210 884\n",
      "Test Query 34: 971 629 699\n",
      "Test Query 35: 885 552 898\n",
      "Test Query 36: 875 262 523\n",
      "Test Query 37: 738 604 961\n",
      "Test Query 38: 279 289 564\n",
      "Test Query 39: 771 364 979\n",
      "Test Query 40: 802 263 100\n",
      "Test Query 41: 538 589 371\n",
      "Test Query 42: 877 444 103\n",
      "Test Query 43: 860 834 392\n",
      "Test Query 44: 753 970 406\n",
      "Test Query 45: 643 455 510\n",
      "Test Query 46: 4 826 970\n",
      "Test Query 47: 280 851 510\n",
      "Test Query 48: 149 684 604\n",
      "Test Query 49: 413 850 247\n",
      "Test Query 50: 448 713 60\n",
      "Test Query 51: 571 130 38\n",
      "Test Query 52: 296 763 593\n",
      "Test Query 53: 57 959 456\n",
      "Test Query 54: 432 95 180\n",
      "Test Query 55: 381 626 311\n",
      "Test Query 56: 506 503 834\n",
      "Test Query 57: 439 239 576\n",
      "Test Query 58: 213 55 341\n",
      "Test Query 59: 818 877 103\n",
      "Test Query 60: 977 599 213\n",
      "Test Query 61: 484 595 894\n",
      "Test Query 62: 936 609 628\n",
      "Test Query 63: 239 439 576\n",
      "Test Query 64: 616 389 829\n",
      "Test Query 65: 553 922 344\n",
      "Test Query 66: 807 794 822\n",
      "Test Query 67: 670 476 249\n",
      "Test Query 68: 902 813 776\n",
      "Test Query 69: 416 519 678\n",
      "Test Query 70: 321 511 992\n",
      "Test Query 71: 906 32 153\n",
      "Test Query 72: 487 639 50\n",
      "Test Query 73: 562 975 826\n",
      "Test Query 74: 446 498 503\n",
      "Test Query 75: 520 86 422\n",
      "Test Query 76: 283 217 698\n",
      "Test Query 77: 419 876 689\n",
      "Test Query 78: 868 343 14\n",
      "Test Query 79: 290 843 940\n",
      "Test Query 80: 627 962 317\n",
      "Test Query 81: 715 594 689\n",
      "Test Query 82: 952 340 330\n",
      "Test Query 83: 994 107 23\n",
      "Test Query 84: 873 961 596\n",
      "Test Query 85: 856 859 253\n",
      "Test Query 86: 234 941 72\n",
      "Test Query 87: 422 860 834\n",
      "Test Query 88: 510 79 433\n",
      "Test Query 89: 242 205 649\n",
      "Test Query 90: 426 613 659\n",
      "Test Query 91: 408 333 444\n",
      "Test Query 92: 384 697 193\n",
      "Test Query 93: 415 386 721\n",
      "Test Query 94: 923 47 529\n",
      "Test Query 95: 628 325 864\n",
      "Test Query 96: 313 48 88\n",
      "Test Query 97: 991 861 863\n",
      "Test Query 98: 533 808 491\n",
      "Test Query 99: 320 494 213\n",
      "Test Query 100: 705 649 972\n",
      "Test Query 101: 329 787 817\n",
      "Test Query 102: 342 709 455\n",
      "Test Query 103: 766 750 645\n",
      "Test Query 104: 754 640 133\n",
      "Test Query 105: 311 237 127\n",
      "Test Query 106: 702 462 487\n",
      "Test Query 107: 80 480 820\n",
      "Test Query 108: 904 553 338\n",
      "Test Query 109: 606 992 765\n",
      "Test Query 110: 503 566 396\n",
      "Test Query 111: 914 330 250\n",
      "Test Query 112: 529 907 156\n",
      "Test Query 113: 669 368 753\n",
      "Test Query 114: 370 388 642\n",
      "Test Query 115: 800 573 185\n",
      "Test Query 116: 569 236 988\n",
      "Test Query 117: 997 228 539\n",
      "Test Query 118: 966 730 849\n",
      "Test Query 119: 518 919 503\n",
      "Test Query 120: 376 719 675\n",
      "Test Query 121: 384 348 724\n",
      "Test Query 122: 368 821 316\n",
      "Test Query 123: 39 219 434\n",
      "Test Query 124: 215 324 312\n",
      "Test Query 125: 944 127 628\n",
      "Test Query 126: 248 419 876\n",
      "Test Query 127: 642 796 399\n",
      "Test Query 128: 902 662 151\n",
      "Test Query 129: 540 312 324\n",
      "Test Query 130: 243 838 703\n",
      "Test Query 131: 683 543 809\n",
      "Test Query 132: 723 721 965\n",
      "Test Query 133: 254 926 37\n",
      "Test Query 134: 79 905 744\n",
      "Test Query 135: 406 663 382\n",
      "Test Query 136: 557 812 46\n",
      "Test Query 137: 928 777 543\n",
      "Test Query 138: 286 758 838\n",
      "Test Query 139: 503 836 758\n",
      "Test Query 140: 975 348 688\n",
      "Test Query 141: 803 704 2\n",
      "Test Query 142: 374 755 113\n",
      "Test Query 143: 466 291 502\n",
      "Test Query 144: 665 311 253\n",
      "Test Query 145: 435 696 680\n",
      "Test Query 146: 943 122 254\n",
      "Test Query 147: 262 243 892\n",
      "Test Query 148: 453 154 692\n",
      "Test Query 149: 938 93 793\n",
      "Test Query 150: 459 947 901\n",
      "Test Query 151: 797 30 2\n",
      "Test Query 152: 724 384 826\n",
      "Test Query 153: 747 957 661\n",
      "Test Query 154: 42 226 237\n",
      "Test Query 155: 299 441 756\n",
      "Test Query 156: 201 682 931\n",
      "Test Query 157: 592 276 576\n",
      "Test Query 158: 927 943 210\n",
      "Test Query 159: 759 758 215\n",
      "Test Query 160: 404 287 632\n",
      "Test Query 161: 498 503 154\n",
      "Test Query 162: 585 193 414\n",
      "Test Query 163: 253 320 546\n",
      "Test Query 164: 621 530 473\n",
      "Test Query 165: 928 971 375\n",
      "Test Query 166: 558 940 61\n",
      "Test Query 167: 414 856 82\n",
      "Test Query 168: 309 270 898\n",
      "Test Query 169: 444 65 373\n",
      "Test Query 170: 431 152 411\n",
      "Test Query 171: 372 917 357\n",
      "Test Query 172: 720 985 417\n",
      "Test Query 173: 887 713 293\n",
      "Test Query 174: 373 444 970\n",
      "Test Query 175: 766 326 848\n",
      "Test Query 176: 847 794 807\n",
      "Test Query 177: 926 644 674\n",
      "Test Query 178: 939 440 582\n",
      "Test Query 179: 714 354 776\n",
      "Test Query 180: 323 412 529\n",
      "Test Query 181: 255 420 510\n",
      "Test Query 182: 502 722 583\n",
      "Test Query 183: 737 36 451\n",
      "Test Query 184: 267 154 498\n",
      "Test Query 185: 932 982 551\n",
      "Test Query 186: 894 616 718\n",
      "Test Query 187: 162 272 651\n",
      "Test Query 188: 626 990 238\n",
      "Test Query 189: 755 113 344\n",
      "Test Query 190: 554 664 72\n",
      "Test Query 191: 968 522 162\n",
      "Test Query 192: 996 24 406\n",
      "Test Query 193: 742 420 930\n",
      "Test Query 194: 836 73 612\n",
      "Test Query 195: 593 51 123\n",
      "Test Query 196: 295 683 433\n",
      "Test Query 197: 977 504 861\n",
      "Test Query 198: 205 165 649\n",
      "Test Query 199: 687 352 912\n",
      "Test Query 200: 963 332 473\n",
      "Test Query 201: 683 543 928\n",
      "Test Query 202: 901 947 625\n",
      "Test Query 203: 60 827 507\n",
      "Test Query 204: 804 200 678\n",
      "Test Query 205: 466 291 502\n",
      "Test Query 206: 594 708 447\n",
      "Test Query 207: 331 24 83\n",
      "Test Query 208: 419 51 876\n",
      "Test Query 209: 727 4 149\n",
      "Test Query 210: 686 885 552\n",
      "Test Query 211: 218 220 156\n",
      "Test Query 212: 985 269 669\n",
      "Test Query 213: 963 614 581\n",
      "Test Query 214: 738 393 348\n",
      "Test Query 215: 658 808 256\n",
      "Test Query 216: 249 749 177\n",
      "Test Query 217: 409 679 700\n",
      "Test Query 218: 391 680 965\n",
      "Test Query 219: 92 897 870\n",
      "Test Query 220: 629 452 699\n",
      "Test Query 221: 703 776 4\n",
      "Test Query 222: 851 750 583\n",
      "Test Query 223: 601 381 253\n",
      "Test Query 224: 168 587 760\n",
      "Test Query 225: 452 345 253\n",
      "Test Query 226: 815 532 621\n",
      "Test Query 227: 762 982 689\n",
      "Test Query 228: 88 489 827\n",
      "Test Query 229: 515 480 180\n",
      "Test Query 230: 262 506 243\n",
      "Test Query 231: 348 393 604\n",
      "Test Query 232: 843 290 940\n",
      "Test Query 233: 352 983 902\n",
      "Test Query 234: 245 92 897\n",
      "Test Query 235: 605 455 376\n",
      "Test Query 236: 357 917 713\n",
      "Test Query 237: 587 760 132\n",
      "Test Query 238: 960 637 824\n",
      "Test Query 239: 350 970 300\n",
      "Test Query 240: 963 614 581\n",
      "Test Query 241: 420 374 673\n",
      "Test Query 242: 978 388 268\n",
      "Test Query 243: 158 801 655\n",
      "Test Query 244: 390 48 24\n",
      "Test Query 245: 289 785 502\n",
      "Test Query 246: 555 358 92\n",
      "Test Query 247: 940 583 336\n",
      "Test Query 248: 155 169 355\n",
      "Test Query 249: 482 870 240\n",
      "Test Query 250: 659 624 426\n",
      "Test Query 251: 366 198 122\n",
      "Test Query 252: 607 128 406\n",
      "Test Query 253: 636 100 649\n",
      "Test Query 254: 113 483 351\n",
      "Test Query 255: 158 801 655\n",
      "Test Query 256: 632 404 590\n",
      "Test Query 257: 650 668 147\n",
      "Test Query 258: 4 604 691\n",
      "Test Query 259: 916 476 177\n",
      "Test Query 260: 707 114 866\n",
      "Test Query 261: 204 649 981\n",
      "Test Query 262: 462 808 395\n",
      "Test Query 263: 544 250 330\n",
      "Test Query 264: 231 715 642\n",
      "Test Query 265: 908 886 585\n",
      "Test Query 266: 166 317 612\n",
      "Test Query 267: 711 170 184\n",
      "Test Query 268: 517 843 290\n",
      "Test Query 269: 922 848 225\n",
      "Test Query 270: 945 350 649\n",
      "Test Query 271: 299 441 219\n",
      "Test Query 272: 537 440 194\n",
      "Test Query 273: 438 340 75\n",
      "Test Query 274: 2 461 827\n",
      "Test Query 275: 970 638 24\n",
      "Test Query 276: 142 635 827\n",
      "Test Query 277: 236 746 976\n",
      "Test Query 278: 278 72 330\n",
      "Test Query 279: 766 645 568\n",
      "Test Query 280: 915 61 752\n",
      "Test Query 281: 464 965 709\n",
      "Test Query 282: 834 860 422\n",
      "Test Query 283: 810 590 589\n",
      "Test Query 284: 672 972 954\n",
      "Test Query 285: 453 162 272\n",
      "Test Query 286: 812 859 333\n",
      "Test Query 287: 32 906 153\n",
      "Test Query 288: 509 354 827\n",
      "Test Query 289: 271 568 754\n",
      "Test Query 290: 794 807 847\n",
      "Test Query 291: 563 411 228\n",
      "Test Query 292: 409 20 700\n",
      "Test Query 293: 258 867 107\n",
      "Test Query 294: 871 971 929\n",
      "Test Query 295: 675 978 703\n",
      "Test Query 296: 486 420 326\n",
      "Test Query 297: 324 540 312\n",
      "Test Query 298: 837 737 643\n",
      "Test Query 299: 471 911 76\n",
      "Test Query 300: 849 966 162\n",
      "Test Query 301: 917 357 713\n",
      "Test Query 302: 325 320 573\n",
      "Test Query 303: 247 46 110\n",
      "Test Query 304: 792 298 170\n",
      "Test Query 305: 499 286 707\n",
      "Test Query 306: 478 599 419\n",
      "Test Query 307: 221 259 980\n",
      "Test Query 308: 613 426 60\n",
      "Test Query 309: 388 865 96\n",
      "Test Query 310: 754 527 250\n",
      "Test Query 311: 648 832 867\n",
      "Test Query 312: 408 333 444\n",
      "Test Query 313: 708 594 642\n",
      "Test Query 314: 474 903 460\n",
      "Test Query 315: 895 228 584\n",
      "Test Query 316: 781 670 470\n",
      "Test Query 317: 68 475 848\n",
      "Test Query 318: 503 506 337\n",
      "Test Query 319: 644 876 35\n",
      "Test Query 320: 779 196 206\n",
      "Test Query 321: 777 727 419\n",
      "Test Query 322: 646 416 983\n",
      "Test Query 323: 405 168 382\n",
      "Test Query 324: 351 162 272\n",
      "Test Query 325: 913 621 511\n",
      "Test Query 326: 878 686 847\n",
      "Test Query 327: 983 770 528\n",
      "Test Query 328: 677 83 917\n",
      "Test Query 329: 683 543 197\n",
      "Test Query 330: 213 880 362\n",
      "Test Query 331: 900 503 128\n",
      "Test Query 332: 263 409 700\n",
      "Test Query 333: 999 105 691\n",
      "Test Query 334: 281 503 561\n",
      "Test Query 335: 608 41 470\n",
      "Test Query 336: 491 942 851\n",
      "Test Query 337: 664 274 152\n",
      "Test Query 338: 287 404 632\n",
      "Test Query 339: 623 162 626\n",
      "Test Query 340: 516 991 476\n",
      "Test Query 341: 260 489 839\n",
      "Test Query 342: 107 23 649\n",
      "Test Query 343: 346 981 204\n",
      "Test Query 344: 723 721 965\n",
      "Test Query 345: 307 608 535\n",
      "Test Query 346: 379 948 164\n",
      "Test Query 347: 619 18 801\n",
      "Test Query 348: 971 485 649\n",
      "Test Query 349: 784 277 899\n",
      "Test Query 350: 649 292 574\n",
      "Test Query 351: 840 626 442\n",
      "Test Query 352: 857 642 222\n",
      "Test Query 353: 638 549 246\n",
      "Test Query 354: 472 38 610\n",
      "Test Query 355: 382 193 928\n",
      "Test Query 356: 757 944 299\n",
      "Test Query 357: 671 244 850\n",
      "Test Query 358: 676 414 5\n",
      "Test Query 359: 637 960 139\n",
      "Test Query 360: 859 856 514\n",
      "Test Query 361: 701 647 669\n",
      "Test Query 362: 889 497 577\n",
      "Test Query 363: 527 389 342\n",
      "Test Query 364: 419 249 217\n",
      "Test Query 365: 990 238 445\n",
      "Test Query 366: 990 238 445\n",
      "Test Query 367: 500 444 333\n",
      "Test Query 368: 766 991 645\n",
      "Test Query 369: 206 486 871\n",
      "Test Query 370: 869 23 83\n",
      "Test Query 371: 348 393 121\n",
      "Test Query 372: 263 193 700\n",
      "Test Query 373: 591 113 755\n",
      "Test Query 374: 286 403 689\n",
      "Test Query 375: 214 392 292\n",
      "Test Query 376: 284 519 806\n",
      "Test Query 377: 713 399 142\n",
      "Test Query 378: 524 2 126\n",
      "Test Query 379: 407 665 972\n",
      "Test Query 380: 529 639 50\n",
      "Test Query 381: 561 912 130\n",
      "Test Query 382: 475 269 399\n",
      "Test Query 383: 473 66 454\n",
      "Test Query 384: 894 616 653\n",
      "Test Query 385: 689 835 867\n",
      "Test Query 386: 510 304 559\n",
      "Test Query 387: 667 560 840\n",
      "Test Query 388: 367 57 422\n",
      "Test Query 389: 929 649 871\n",
      "Test Query 390: 327 618 198\n",
      "Test Query 391: 460 236 668\n",
      "Test Query 392: 232 616 415\n",
      "Test Query 393: 485 971 882\n",
      "Test Query 394: 203 948 91\n",
      "Test Query 395: 806 519 620\n",
      "Test Query 396: 401 315 283\n",
      "Test Query 397: 450 419 749\n",
      "Test Query 398: 615 732 623\n",
      "Test Query 399: 88 23 392\n",
      "Test Query 400: 719 713 589\n",
      "Results for test queries saved to vector_1010_1642_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "# 讀取資料\n",
    "train_questions = pd.read_csv('train_question.csv')\n",
    "test_questions = pd.read_csv('test_question.csv')\n",
    "documents_data = pd.read_csv('documents_data.csv')\n",
    "\n",
    "# 定義預處理函數\n",
    "def clean_html(document_html, is_html=False):\n",
    "    if is_html:\n",
    "        soup = BeautifulSoup(document_html, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "    else:\n",
    "        text = document_html\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    return text\n",
    "\n",
    "# 預處理文檔和訓練集\n",
    "documents_data['cleaned_text'] = documents_data['Document_HTML'].apply(lambda x: clean_html(x, is_html=True))\n",
    "train_questions['cleaned_question'] = train_questions['Question'].apply(clean_html)\n",
    "\n",
    "# 預處理測試集\n",
    "test_questions['cleaned_question'] = test_questions['Question'].apply(clean_html)\n",
    "\n",
    "# 建立詞彙表（僅根據訓練集和文檔中的詞彙）\n",
    "docs_tokens = documents_data['cleaned_text'].apply(lambda x: x.split())\n",
    "train_questions_tokens = train_questions['cleaned_question'].apply(lambda x: x.split())\n",
    "all_tokens = [token for tokens in docs_tokens for token in tokens] + [token for tokens in train_questions_tokens for token in tokens]\n",
    "vocabulary = list(set(all_tokens))\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# 建立詞彙到索引的映射\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# 建立文檔-詞矩陣 (Document-Term Matrix)\n",
    "num_docs = len(documents_data)\n",
    "DTM = np.zeros((num_docs, vocab_size))\n",
    "\n",
    "for i, tokens in enumerate(docs_tokens):\n",
    "    token_indices = [word_to_index[token] for token in tokens if token in word_to_index]\n",
    "    counts = np.bincount(token_indices, minlength=vocab_size)\n",
    "    DTM[i] = counts\n",
    "\n",
    "# 建立訓練查詢-詞矩陣 (Query-Term Matrix) for train\n",
    "num_train_queries = len(train_questions)\n",
    "QTM_train = np.zeros((num_train_queries, vocab_size))\n",
    "\n",
    "for i, tokens in enumerate(train_questions_tokens):\n",
    "    token_indices = [word_to_index[token] for token in tokens if token in word_to_index]\n",
    "    counts = np.bincount(token_indices, minlength=vocab_size)\n",
    "    QTM_train[i] = counts\n",
    "\n",
    "# 計算IDF（使用訓練集和文檔）\n",
    "DF = np.sum(DTM > 0, axis=0)  # 包含每個詞的文檔數\n",
    "N = num_docs\n",
    "IDF = np.log(N / (1 + DF))  # 防止除以0\n",
    "\n",
    "# 計算TF\n",
    "doc_lengths = np.sum(DTM, axis=1, keepdims=True)  # 文檔中詞的總數\n",
    "doc_lengths[doc_lengths == 0] = 1  # 防止除以0\n",
    "TF_docs = DTM / doc_lengths\n",
    "\n",
    "train_query_lengths = np.sum(QTM_train, axis=1, keepdims=True)\n",
    "train_query_lengths[train_query_lengths == 0] = 1\n",
    "TF_train_queries = QTM_train / train_query_lengths\n",
    "\n",
    "# 計算TF-IDF for documents and train queries\n",
    "TFIDF_docs = TF_docs * IDF\n",
    "TFIDF_train_queries = TF_train_queries * IDF\n",
    "\n",
    "# 正規化TF-IDF向量\n",
    "norms_docs = np.linalg.norm(TFIDF_docs, axis=1, keepdims=True)\n",
    "norms_docs[norms_docs == 0] = 1\n",
    "normalized_TFIDF_docs = TFIDF_docs / norms_docs\n",
    "\n",
    "norms_train_queries = np.linalg.norm(TFIDF_train_queries, axis=1, keepdims=True)\n",
    "norms_train_queries[norms_train_queries == 0] = 1\n",
    "normalized_TFIDF_train_queries = TFIDF_train_queries / norms_train_queries\n",
    "\n",
    "# 計算餘弦相似度矩陣 for train queries\n",
    "similarity_matrix_train = np.dot(normalized_TFIDF_train_queries, normalized_TFIDF_docs.T)\n",
    "\n",
    "# 對每個訓練查詢找到最相似的前三個文檔\n",
    "top_k = 3\n",
    "top_k_indices_train = np.argsort(similarity_matrix_train, axis=1)[:, -top_k:][:, ::-1]\n",
    "predicted_docs_train = documents_data['Document ID'].values\n",
    "\n",
    "# 提取訓練查詢的預測文檔ID\n",
    "results_train = [predicted_docs_train[indices] for indices in top_k_indices_train]\n",
    "\n",
    "# 現在對測試查詢進行預測\n",
    "# 建立測試查詢-詞矩陣 (Query-Term Matrix) for test\n",
    "test_questions_tokens = test_questions['cleaned_question'].apply(lambda x: x.split())\n",
    "num_test_queries = len(test_questions)\n",
    "QTM_test = np.zeros((num_test_queries, vocab_size))\n",
    "\n",
    "for i, tokens in enumerate(test_questions_tokens):\n",
    "    token_indices = [word_to_index[token] for token in tokens if token in word_to_index]\n",
    "    counts = np.bincount(token_indices, minlength=vocab_size)\n",
    "    QTM_test[i] = counts\n",
    "\n",
    "# 計算TF for test queries\n",
    "test_query_lengths = np.sum(QTM_test, axis=1, keepdims=True)\n",
    "test_query_lengths[test_query_lengths == 0] = 1\n",
    "TF_test_queries = QTM_test / test_query_lengths\n",
    "\n",
    "# 計算TF-IDF for test queries\n",
    "TFIDF_test_queries = TF_test_queries * IDF\n",
    "\n",
    "# 正規化TF-IDF向量 for test queries\n",
    "norms_test_queries = np.linalg.norm(TFIDF_test_queries, axis=1, keepdims=True)\n",
    "norms_test_queries[norms_test_queries == 0] = 1\n",
    "normalized_TFIDF_test_queries = TFIDF_test_queries / norms_test_queries\n",
    "\n",
    "# 計算餘弦相似度矩陣 for test queries\n",
    "similarity_matrix_test = np.dot(normalized_TFIDF_test_queries, normalized_TFIDF_docs.T)\n",
    "\n",
    "# 對每個測試查詢找到最相似的前三個文檔\n",
    "top_k_indices_test = np.argsort(similarity_matrix_test, axis=1)[:, -top_k:][:, ::-1]\n",
    "predicted_docs_test = documents_data['Document ID'].values\n",
    "\n",
    "# 提取測試查詢的預測文檔ID\n",
    "results_test = [predicted_docs_test[indices] for indices in top_k_indices_test]\n",
    "\n",
    "# 保存結果 for test queries\n",
    "now = datetime.datetime.now()\n",
    "formatted_time = now.strftime(\"%m%d_%H%M\")\n",
    "csv_filename = f'vector_{formatted_time}_test.csv'\n",
    "result_df_test = pd.DataFrame({\n",
    "    \"index\": list(range(1, len(results_test) + 1)),\n",
    "    \"answer\": [' '.join(map(str, doc_ids)) for doc_ids in results_test]\n",
    "})\n",
    "\n",
    "result_df_test.to_csv(csv_filename, index=False)\n",
    "\n",
    "# 輸出測試查詢結果\n",
    "for i, doc_ids in enumerate(results_test):\n",
    "    answer = ' '.join(map(str, doc_ids))\n",
    "    print(f\"Test Query {i+1}: {answer}\")\n",
    "\n",
    "print(f\"Results for test queries saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_question.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 讀取資料\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_questions \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_question.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m test_questions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_question.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m documents_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_question.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "# 讀取資料\n",
    "train_questions = pd.read_csv('train_question.csv')\n",
    "test_questions = pd.read_csv('test_question.csv')\n",
    "documents_data = pd.read_csv('documents_data.csv')\n",
    "\n",
    "# 停用詞列表（可根據需要擴充）\n",
    "stop_words = set(['the', 'is', 'in', 'at', 'of', 'and', 'a', 'to'])\n",
    "\n",
    "# 定義文本預處理函數\n",
    "def clean_text(text, is_html=False):\n",
    "    if is_html:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    return text\n",
    "\n",
    "# 預處理文檔和查詢\n",
    "documents_data['cleaned_text'] = documents_data['Document_HTML'].apply(lambda x: clean_text(x, is_html=True))\n",
    "train_questions['cleaned_question'] = train_questions['Question'].apply(clean_text)\n",
    "test_questions['cleaned_question'] = test_questions['Question'].apply(clean_text)\n",
    "\n",
    "# 建立詞彙表（基於訓練集和文檔）\n",
    "docs_tokens = documents_data['cleaned_text'].apply(lambda x: x.split())\n",
    "train_tokens = train_questions['cleaned_question'].apply(lambda x: x.split())\n",
    "\n",
    "# 計算所有詞的出現次數\n",
    "all_tokens = [token for tokens in docs_tokens for token in tokens] + [token for tokens in train_tokens for token in tokens]\n",
    "all_word_counts = pd.Series(all_tokens).value_counts()\n",
    "\n",
    "# 建立詞彙表\n",
    "vocabulary = all_word_counts.index.tolist()\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# 詞彙到索引的映射\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# 構建文檔-詞矩陣 (DTM)\n",
    "num_docs = len(documents_data)\n",
    "DTM = np.zeros((num_docs, vocab_size))\n",
    "doc_lengths = np.zeros(num_docs)\n",
    "\n",
    "for i, tokens in enumerate(docs_tokens):\n",
    "    token_indices = [word_to_index[token] for token in tokens if token in word_to_index]\n",
    "    counts = np.bincount(token_indices, minlength=vocab_size)\n",
    "    DTM[i] = counts\n",
    "    doc_lengths[i] = np.sum(counts)\n",
    "\n",
    "# 計算平均文檔長度\n",
    "avg_doc_length = np.mean(doc_lengths)\n",
    "\n",
    "# 計算IDF（逆文檔頻率）\n",
    "df = np.sum(DTM > 0, axis=0)\n",
    "N = num_docs\n",
    "IDF = np.log((N - df + 0.5) / (df + 0.5) + 1)\n",
    "\n",
    "# 設定 BM25 的參數\n",
    "k1 = 1.5\n",
    "b = 0.75\n",
    "\n",
    "# 計算 BM25 文檔矩陣\n",
    "BM25_docs = np.zeros((num_docs, vocab_size))\n",
    "\n",
    "for i in range(num_docs):\n",
    "    for j in range(vocab_size):\n",
    "        tf = DTM[i, j]\n",
    "        if tf > 0:\n",
    "            numerator = tf * (k1 + 1)\n",
    "            denominator = tf + k1 * (1 - b + b * (doc_lengths[i] / avg_doc_length))\n",
    "            BM25_docs[i, j] = IDF[j] * (numerator / denominator)\n",
    "\n",
    "# 處理測試查詢\n",
    "test_tokens = test_questions['cleaned_question'].apply(lambda x: x.split())\n",
    "num_test_queries = len(test_questions)\n",
    "BM25_test_queries = np.zeros((num_test_queries, vocab_size))\n",
    "\n",
    "for i, tokens in enumerate(test_tokens):\n",
    "    token_indices = [word_to_index[token] for token in tokens if token in word_to_index]\n",
    "    query_term_freq = np.bincount(token_indices, minlength=vocab_size)\n",
    "    BM25_test_queries[i] = query_term_freq * IDF\n",
    "\n",
    "# 計算查詢與文檔的 BM25 相似度\n",
    "similarity_matrix = np.dot(BM25_test_queries, BM25_docs.T)\n",
    "\n",
    "# 對每個查詢找到最相關的前三個文檔\n",
    "top_k = 3\n",
    "top_k_indices = np.argsort(similarity_matrix, axis=1)[:, -top_k:][:, ::-1]\n",
    "predicted_docs = documents_data['Document ID'].values\n",
    "\n",
    "# 提取預測結果\n",
    "results = [predicted_docs[indices] for indices in top_k_indices]\n",
    "\n",
    "# 保存結果\n",
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "csv_filename = f'vector_{now}_test.csv'\n",
    "result_df = pd.DataFrame({\n",
    "    \"index\": range(1, len(results) + 1),\n",
    "    \"answer\": [' '.join(map(str, doc_ids)) for doc_ids in results]\n",
    "})\n",
    "result_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# 輸出結果\n",
    "for i, doc_ids in enumerate(results):\n",
    "    print(f\"Test Query {i+1}: {' '.join(map(str, doc_ids))}\")\n",
    "\n",
    "print(f\"Results saved to {csv_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
