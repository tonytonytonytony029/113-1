{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\richardlin\\conda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\richardlin\\conda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Training Epoch 1/5: 100%|██████████| 313/313 [02:53<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.862026016171367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/5: 100%|██████████| 313/313 [02:50<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 1.1678033562513968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/5: 100%|██████████| 313/313 [02:50<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.690227624255057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/5: 100%|██████████| 313/313 [02:49<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.3440193438801331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/5:  14%|█▍        | 44/313 [00:23<02:27,  1.83it/s]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "#===========================================================\n",
    "# Config 區\n",
    "#===========================================================\n",
    "TRAIN_JSON_PATH = './train.jsonl'\n",
    "TEST_JSON_PATH = './test.jsonl'\n",
    "TEST_IMAGES_JSON_PATH = './test_images.jsonl'\n",
    "TRAIN_IMAGES_DIR = './train_images/train_images'\n",
    "TEST_IMAGES_DIR = './test_images/test_images'\n",
    "OUTPUT_CSV = 'submission.csv'\n",
    "TOP_K = 30\n",
    "EPOCHS = 5\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "TEMPERATURE = 0.07  # CLIP-style temperature\n",
    "\n",
    "#===========================================================\n",
    "# 通用函式\n",
    "#===========================================================\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"讀取 JSONL 檔案並返回資料列表\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def merge_dialogue_messages(dialogue):\n",
    "    \"\"\"將 dialogue 列表中所有 message 合併為單一字串\"\"\"\n",
    "    messages = [turn['message'] for turn in dialogue if turn['message'].strip() != '']\n",
    "    return \" \".join(messages) if messages else \"\"\n",
    "\n",
    "#===========================================================\n",
    "# 圖片與文字預處理 (加入資料增強)\n",
    "#===========================================================\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 資料增強：水平翻轉\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_text(text, tokenizer=bert_tokenizer, max_length=128):\n",
    "    inputs = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "    return inputs\n",
    "\n",
    "#===========================================================\n",
    "# Dataset 定義\n",
    "#===========================================================\n",
    "class DialogueImageTrainDataset(Dataset):\n",
    "    \"\"\"訓練資料集：包含對話與圖片資訊\"\"\"\n",
    "    def __init__(self, data, image_dir, tokenizer, transform):\n",
    "        self.data = data\n",
    "        self.image_dir = image_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        # 將 dialogue 合併為一個文字字串\n",
    "        dialogue_text = merge_dialogue_messages(item['dialogue'])\n",
    "        \n",
    "        photo_path = item['photo_path']\n",
    "        photo_id = item['photo_id']\n",
    "\n",
    "        # 載入圖片\n",
    "        image_path = os.path.join(self.image_dir, photo_path)\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # 編碼文字\n",
    "        text_inputs = encode_text(dialogue_text, self.tokenizer)\n",
    "        \n",
    "        return {\n",
    "            'dialogue_id': item['dialogue_id'],\n",
    "            'photo_id': photo_id,\n",
    "            'image': image,\n",
    "            'input_ids': text_inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_inputs['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "class DialogueTestDataset(Dataset):\n",
    "    \"\"\"測試資料集：僅包含對話 (需用於檢索 test_images)\"\"\"\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        # 將 dialogue 合併成文字字串\n",
    "        dialogue_text = merge_dialogue_messages(item['dialogue'])\n",
    "        text_inputs = encode_text(dialogue_text, self.tokenizer)\n",
    "        \n",
    "        return {\n",
    "            'dialogue_id': item['dialogue_id'],\n",
    "            'input_ids': text_inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_inputs['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "class TestImageDataset(Dataset):\n",
    "    \"\"\"測試集圖片資料集：用於產生所有 test_images 的特徵\"\"\"\n",
    "    def __init__(self, data, image_dir, transform):\n",
    "        self.data = data\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        photo_id = item['photo_id']\n",
    "        photo_path = item['photo_path']\n",
    "        image_path = os.path.join(self.image_dir, photo_path)\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return {\n",
    "            'photo_id': photo_id,\n",
    "            'image': image\n",
    "        }\n",
    "\n",
    "#===========================================================\n",
    "# 模型定義 (Dual Encoder 範例)\n",
    "#===========================================================\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, text_model_name='bert-base-uncased'):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model_name)\n",
    "        \n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        resnet.fc = nn.Identity()\n",
    "        self.image_encoder = resnet\n",
    "        \n",
    "        self.hidden_dim = 2048\n",
    "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, self.hidden_dim)\n",
    "        self.image_proj = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :] \n",
    "        return self.text_proj(cls_emb)\n",
    "    \n",
    "    def encode_image(self, image):\n",
    "        img_emb = self.image_encoder(image)\n",
    "        return self.image_proj(img_emb)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        text_emb = self.encode_text(input_ids, attention_mask)\n",
    "        image_emb = self.encode_image(image)\n",
    "        return text_emb, image_emb\n",
    "\n",
    "#===========================================================\n",
    "# 載入資料\n",
    "#===========================================================\n",
    "train_data = load_jsonl(TRAIN_JSON_PATH)\n",
    "test_data = load_jsonl(TEST_JSON_PATH)\n",
    "test_images_data = load_jsonl(TEST_IMAGES_JSON_PATH)\n",
    "\n",
    "train_dataset = DialogueImageTrainDataset(train_data, TRAIN_IMAGES_DIR, bert_tokenizer, image_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = DialogueTestDataset(test_data, bert_tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_img_dataset = TestImageDataset(test_images_data, TEST_IMAGES_DIR, image_transform)\n",
    "test_img_loader = DataLoader(test_img_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "#===========================================================\n",
    "# 訓練模型 (CLIP-style 對比學習，加入雙向對比)\n",
    "#===========================================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DualEncoder().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 使用學習率排程 (CosineAnnealingLR)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        input_ids = batch['input_ids'].to(device)            # (batch, seq_len)\n",
    "        attention_mask = batch['attention_mask'].to(device)  # (batch, seq_len)\n",
    "        images = batch['image'].to(device)                   # (batch, C, H, W)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向傳遞得到文字與圖片的嵌入\n",
    "        text_emb, image_emb = model(input_ids, attention_mask, images) \n",
    "        # 對 embedding 做 L2 正規化\n",
    "        text_emb_norm = text_emb / text_emb.norm(dim=1, keepdim=True)\n",
    "        image_emb_norm = image_emb / image_emb.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # 計算相似度矩陣 (batch, batch) 並考慮溫度參數\n",
    "        logits = torch.matmul(text_emb_norm, image_emb_norm.T) / TEMPERATURE\n",
    "        labels = torch.arange(logits.size(0)).to(device)\n",
    "\n",
    "        # 雙向對比學習損失計算 (text->image & image->text)\n",
    "        logits_t2i = logits\n",
    "        logits_i2t = logits.T\n",
    "        loss_t2i = criterion(logits_t2i, labels)\n",
    "        loss_i2t = criterion(logits_i2t, labels)\n",
    "\n",
    "        # 將兩個方向的損失平均\n",
    "        loss = (loss_t2i + loss_i2t) / 2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    scheduler.step()  # 更新學習率\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss}\")\n",
    "\n",
    "# 訓練完成後將模型轉為評估模式並保存\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), \"dual_encoder_model.pth\")\n",
    "print(\"Model training completed and saved.\")\n",
    "\n",
    "#===========================================================\n",
    "# 推論 (生成提交檔案)\n",
    "#===========================================================\n",
    "model.eval()\n",
    "\n",
    "# 產生測試圖片嵌入向量 (保存在 device 上)\n",
    "test_image_embeddings = {}\n",
    "with torch.no_grad():\n",
    "    for batch in test_img_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        photo_ids = batch['photo_id']\n",
    "        img_emb = model.encode_image(images)  # (batch, hidden_dim)\n",
    "        # 推論階段可使用正規化以保持一致\n",
    "        img_emb = img_emb / img_emb.norm(dim=1, keepdim=True)\n",
    "        for pid, emb in zip(photo_ids, img_emb):\n",
    "            test_image_embeddings[pid] = emb\n",
    "\n",
    "predictions = []\n",
    "\n",
    "all_img_ids = list(test_image_embeddings.keys())\n",
    "if len(all_img_ids) > 0:\n",
    "    all_img_embs = torch.stack([test_image_embeddings[i] for i in all_img_ids]).to(device)  # (num_imgs, hidden_dim)\n",
    "    # 已正規化過\n",
    "    K = min(TOP_K, len(all_img_ids))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            dialogue_id = batch['dialogue_id'].item()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            text_emb = model.encode_text(input_ids, attention_mask) # (1, hidden_dim)\n",
    "            text_emb_norm = text_emb / text_emb.norm(dim=1, keepdim=True)\n",
    "            \n",
    "            similarity = torch.matmul(text_emb_norm, all_img_embs.T).squeeze(0) # (num_imgs,)\n",
    "            topk_vals, topk_indices = torch.topk(similarity, K)\n",
    "            top_photo_ids = [all_img_ids[i] for i in topk_indices.cpu().tolist()]\n",
    "            \n",
    "            # 將所有 top_photo_ids 加入 predictions 清單\n",
    "            for pid in top_photo_ids:\n",
    "                predictions.append((dialogue_id, pid))\n",
    "else:\n",
    "    print(\"Warning: No test images found. No predictions can be made.\")\n",
    "\n",
    "#===========================================================\n",
    "# 合併同一 dialogue_id 的 photo_id 並輸出\n",
    "#===========================================================\n",
    "dialogue_to_photos = defaultdict(list)\n",
    "for d_id, p_id in predictions:\n",
    "    dialogue_to_photos[d_id].append(p_id)\n",
    "\n",
    "def generate_submission(dialogue_to_photos, output_path):\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['dialogue_id', 'photo_id'])\n",
    "        for d_id, p_ids in dialogue_to_photos.items():\n",
    "            photo_str = \" \".join(p_ids)\n",
    "            writer.writerow([d_id, photo_str])\n",
    "\n",
    "if dialogue_to_photos:\n",
    "    generate_submission(dialogue_to_photos, OUTPUT_CSV)\n",
    "    print(f\"Submission file saved to {OUTPUT_CSV}\")\n",
    "else:\n",
    "    print(\"No predictions were made.\")\n",
    "\n",
    "#===========================================================\n",
    "# 計算 Recall@30 (若需要可執行)\n",
    "#===========================================================\n",
    "dialogue_to_gt = {}\n",
    "for item in test_data:\n",
    "    d_id = item['dialogue_id']\n",
    "    gt_pid = item['photo_id']\n",
    "    dialogue_to_gt[d_id] = gt_pid\n",
    "\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for d_id, gt_pid in dialogue_to_gt.items():\n",
    "    total_count += 1\n",
    "    predicted_pids = dialogue_to_photos.get(d_id, [])\n",
    "    if gt_pid in predicted_pids:\n",
    "        correct_count += 1\n",
    "\n",
    "if total_count > 0:\n",
    "    recall_at_30 = correct_count / total_count\n",
    "    print(f\"Recall@30: {recall_at_30}\")\n",
    "else:\n",
    "    print(\"No ground truth available for recall calculation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================\n",
    "# 計算 Recall@30\n",
    "#===========================================================\n",
    "# 假設 test_data 中每一筆都有一個正確的 photo_id\n",
    "dialogue_to_gt = {}\n",
    "for item in test_data:\n",
    "    d_id = item['dialogue_id']\n",
    "    gt_pid = item['photo_id']\n",
    "    dialogue_to_gt[d_id] = gt_pid\n",
    "\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for d_id, gt_pid in dialogue_to_gt.items():\n",
    "    total_count += 1\n",
    "    predicted_pids = dialogue_to_photos.get(d_id, [])\n",
    "    if gt_pid in predicted_pids:\n",
    "        correct_count += 1\n",
    "\n",
    "recall_at_30 = correct_count / total_count if t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
